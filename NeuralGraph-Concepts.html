<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>NeuralGraph — How It Works</title>
    <style>
        @page {
            size: A4;
            margin: 40px 48px;
        }

        * { margin: 0; padding: 0; box-sizing: border-box; }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Helvetica Neue', 'Inter', sans-serif;
            color: #e8e8f0;
            background: #0f0f1a;
            font-size: 11pt;
            line-height: 1.6;
            -webkit-print-color-adjust: exact;
            print-color-adjust: exact;
        }

        .header {
            text-align: center;
            padding: 40px 0 32px;
            border-bottom: 1px solid rgba(139, 92, 246, 0.3);
            margin-bottom: 32px;
            background: radial-gradient(ellipse at center, rgba(139, 92, 246, 0.08) 0%, transparent 70%);
        }

        .header img {
            width: 80px;
            height: 80px;
            margin-bottom: 16px;
        }

        .header h1 {
            font-size: 32pt;
            font-weight: 700;
            letter-spacing: -0.02em;
            margin-bottom: 6px;
            color: #e8e8f0;
        }

        .header h1 span {
            background: linear-gradient(135deg, #4aa3df, #8b5cf6, #e84393);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .header .lead {
            font-size: 11pt;
            color: #8888a0;
            max-width: 520px;
            margin: 0 auto;
            line-height: 1.7;
        }

        .header .meta {
            font-size: 9pt;
            color: #555570;
            margin-top: 10px;
        }

        h2 {
            font-size: 16pt;
            font-weight: 600;
            color: #e8e8f0;
            margin-bottom: 12px;
            padding-top: 28px;
            display: flex;
            align-items: center;
            gap: 10px;
        }

        h2 .icon {
            width: 30px;
            height: 30px;
            line-height: 30px;
            text-align: center;
            border-radius: 8px;
            background: rgba(139, 92, 246, 0.15);
            font-size: 14px;
            flex-shrink: 0;
        }

        .section-divider {
            height: 1px;
            background: linear-gradient(90deg, transparent, rgba(139, 92, 246, 0.2), transparent);
            margin-top: 28px;
        }

        p {
            margin-bottom: 10px;
            color: #a0a0b8;
        }

        strong {
            color: #e8e8f0;
            font-weight: 600;
        }

        em {
            color: #8b5cf6;
            font-style: normal;
            font-weight: 500;
        }

        .diagram {
            background: rgba(0, 0, 0, 0.4);
            border: 1px solid rgba(255, 255, 255, 0.06);
            border-radius: 10px;
            padding: 18px 22px;
            margin: 16px 0;
            font-family: 'SF Mono', 'Menlo', 'Consolas', monospace;
            font-size: 9pt;
            line-height: 1.8;
            white-space: pre;
            overflow: visible;
            color: #666680;
            page-break-inside: avoid;
        }

        .diagram .node { color: #4aa3df; font-weight: 600; }
        .diagram .trigger { color: #8b5cf6; }
        .diagram .label { color: #e84393; }
        .diagram .score { color: #22c55e; }
        .diagram .space { color: #f59e0b; font-weight: 600; }
        .diagram .comment { color: #555570; }
        .diagram .edge { color: #666680; }

        .section {
            margin-bottom: 8px;
            page-break-inside: avoid;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 16px 0;
            font-size: 10pt;
            page-break-inside: avoid;
        }

        th {
            text-align: left;
            padding: 10px 14px;
            font-size: 8pt;
            font-weight: 700;
            text-transform: uppercase;
            letter-spacing: 0.06em;
            color: #8888a0;
            background: rgba(255, 255, 255, 0.03);
            border-bottom: 1px solid rgba(255, 255, 255, 0.08);
        }

        td {
            padding: 10px 14px;
            color: #a0a0b8;
            border-bottom: 1px solid rgba(255, 255, 255, 0.04);
            vertical-align: top;
        }

        td:first-child {
            color: #e8e8f0;
            font-weight: 500;
        }

        tr:last-child td {
            border-bottom: none;
        }

        tr.highlight td {
            background: rgba(139, 92, 246, 0.08);
        }

        tr.highlight td:first-child {
            color: #8b5cf6;
            font-weight: 600;
        }

        /* Flow list */
        .flow-list {
            list-style: none;
            counter-reset: flow;
            margin: 16px 0;
        }

        .flow-list li {
            counter-increment: flow;
            padding: 8px 0 8px 40px;
            position: relative;
            color: #a0a0b8;
        }

        .flow-list li::before {
            content: counter(flow);
            position: absolute;
            left: 0;
            width: 24px;
            height: 24px;
            line-height: 24px;
            text-align: center;
            border-radius: 50%;
            background: rgba(139, 92, 246, 0.15);
            color: #8b5cf6;
            font-size: 9pt;
            font-weight: 700;
        }

        .flow-list li strong {
            color: #e8e8f0;
        }

        /* Callout */
        .callout {
            background: rgba(139, 92, 246, 0.06);
            border: 1px solid rgba(139, 92, 246, 0.15);
            border-radius: 10px;
            padding: 14px 18px;
            margin: 16px 0;
            font-size: 10pt;
            color: #a0a0b8;
            page-break-inside: avoid;
        }

        .callout strong {
            color: #8b5cf6;
        }

        /* Footer */
        .footer {
            text-align: center;
            padding-top: 28px;
            border-top: 1px solid rgba(255, 255, 255, 0.06);
            margin-top: 32px;
            font-size: 9pt;
            color: #555570;
        }

        .page-break {
            page-break-before: always;
        }
    </style>
</head>
<body>

<div class="header">
    <img src="public/assets/logo-dark.png" alt="NeuralGraph">
    <h1>Neural<span>Graph</span></h1>
    <p class="lead">
        A context engine that stores knowledge as interconnected graphs and retrieves the right context at the right time. Not a chatbot framework. Not a RAG pipeline. A structured memory layer for AI.
    </p>
    <p class="meta">Confidential &mdash; NeuralGraph LLC &mdash; 2026</p>
</div>

<!-- Graph Memory -->
<div class="section">
    <h2><span class="icon">&#x25C8;</span> Graph Memory</h2>
    <p>
        Most AI memory systems store flat chunks of text and retrieve them with vector similarity. That works for simple recall, but it loses structure. <strong>NeuralGraph stores knowledge as a graph</strong> — nodes represent discrete pieces of knowledge, edges represent relationships between them.
    </p>
    <p>
        A node can be anything: a fact, a person, a preference, a concept, an event. Each node has a type, importance score, and structured data. Edges carry meaning — <strong>RELATED_TO</strong>, <strong>CONTRADICTS</strong>, <strong>SUBTOPIC_OF</strong>, <strong>SUPERSEDES</strong> — so the system understands not just what it knows, but how things connect.
    </p>
    <div class="diagram"><span class="comment">A conversation about a user's work life produces:</span>

<span class="node">[David]</span> <span class="edge">--WORKS_AT--></span> <span class="node">[Acme Corp]</span>
   <span class="edge">|</span>                          <span class="edge">|</span>
   <span class="edge">MANAGES</span>                   <span class="edge">SUBTOPIC_OF</span>
   <span class="edge">|</span>                          <span class="edge">|</span>
<span class="node">[Team Alpha]</span>             <span class="node">[Tech Industry]</span>
   <span class="edge">|</span>
   <span class="edge">RELATED_TO</span>
   <span class="edge">|</span>
<span class="node">[Q3 Launch]</span> <span class="edge">--CAUSES--></span> <span class="node">[Stress]</span></div>
    <p>
        When new information arrives, the graph updates. If David changes jobs, the old <strong>WORKS_AT</strong> edge gets superseded — not deleted. The graph maintains history while keeping current state clear.
    </p>
    <div class="section-divider"></div>
</div>

<!-- Triggers -->
<div class="section">
    <h2><span class="icon">&#x26A1;</span> Triggers</h2>
    <p>
        This is what makes NeuralGraph different. <strong>Triggers are semantic hooks</strong> — short phrases extracted alongside each node that describe when that knowledge should surface. They're not keywords. They're intent signals.
    </p>
    <p>
        When a user says "I'm stressed about the launch," the system doesn't just vector-search for similar text. It scans for triggers. The node <strong>[Q3 Launch]</strong> might have triggers like <em>"launch timeline"</em>, <em>"project deadline"</em>, <em>"work pressure"</em>. The node <strong>[Stress]</strong> might have <em>"feeling overwhelmed"</em>, <em>"anxiety"</em>. Both fire. Both contribute context.
    </p>
    <div class="diagram"><span class="comment">Each node has triggers that control when it surfaces:</span>

<span class="node">[Q3 Launch]</span>
   <span class="trigger">trigger</span>: <span class="label">"launch timeline"</span>    <span class="score">strength: 1.4</span>
   <span class="trigger">trigger</span>: <span class="label">"project deadline"</span>   <span class="score">strength: 1.1</span>
   <span class="trigger">trigger</span>: <span class="label">"work pressure"</span>      <span class="score">strength: 0.9</span>
   <span class="trigger">trigger</span>: <span class="label">"release date"</span>       <span class="score">strength: 0.7</span>

<span class="comment">Triggers strengthen with positive feedback (+0.1, cap 2.0)</span>
<span class="comment">and weaken when irrelevant (-0.05, floor 0.1)</span></div>
    <p>
        Trigger strength is adaptive. When the AI uses a piece of context and it's relevant, the triggers that surfaced it get stronger. When context is irrelevant, those triggers weaken. <strong>The graph learns what matters over time</strong> — no retraining, no manual tuning.
    </p>
    <div class="section-divider"></div>
</div>

<!-- Context Spaces -->
<div class="section">
    <h2><span class="icon">&#x2B21;</span> Context Spaces</h2>
    <p>
        Knowledge lives in <strong>spaces</strong> — isolated graphs with their own schema, ingestion rules, and retrieval config. A space can be anything: a personal memory store, a domain knowledge base, a team wiki, a product catalog.
    </p>
    <p>
        Each space has a <strong>space type</strong> defined in YAML that controls everything: what node types exist, what edges are allowed, how text is extracted into nodes, how nodes are scored during retrieval, and how knowledge decays over time.
    </p>
    <div class="diagram"><span class="space">Space: "memories"</span>        <span class="space">Space: "research"</span>        <span class="space">Space: "personality"</span>
<span class="comment">type: personal_memory</span>     <span class="comment">type: knowledge_base</span>     <span class="comment">type: ai_personality</span>
<span class="edge">┌──────────────┐</span>       <span class="edge">┌──────────────┐</span>       <span class="edge">┌──────────────┐</span>
<span class="edge">│</span> <span class="node">[Max the dog]</span> <span class="edge">│</span>       <span class="edge">│</span> <span class="node">[Quantum ML]</span>  <span class="edge">│</span>       <span class="edge">│</span> <span class="node">[Tone: warm]</span>  <span class="edge">│</span>
<span class="edge">│</span> <span class="node">[Thai food]</span>   <span class="edge">│</span>       <span class="edge">│</span> <span class="node">[RLHF paper]</span>  <span class="edge">│</span>       <span class="edge">│</span> <span class="node">[Curious]</span>     <span class="edge">│</span>
<span class="edge">│</span> <span class="node">[Q3 Launch]</span>   <span class="edge">│</span>       <span class="edge">│</span> <span class="node">[Attention]</span>   <span class="edge">│</span>       <span class="edge">│</span> <span class="node">[Concise]</span>     <span class="edge">│</span>
<span class="edge">└──────────────┘</span>       <span class="edge">└──────────────┘</span>       <span class="edge">└──────────────┘</span></div>
    <p>
        Spaces are <strong>isolated by default, composable at query time</strong>. A single hydration request can search across any combination of spaces — merging a scientist's research graph with their personal memory graph and an AI personality graph in one query. The results are scored and ranked together.
    </p>
    <div class="section-divider"></div>
</div>

<!-- Multi-Space Hydration -->
<div class="section page-break">
    <h2><span class="icon">&#x1F500;</span> Multi-Space Hydration</h2>
    <p>
        At query time, you pass a list of <strong>space_ids</strong> and NeuralGraph searches across all of them simultaneously. Each space contributes nodes, and they're scored together in a single ranked list.
    </p>
    <p>
        Imagine a research assistant that knows both your personal context and a domain knowledge base. When you ask "What papers should I read next?", NeuralGraph pulls from your <strong>personality space</strong> (you prefer practical over theoretical), your <strong>memory space</strong> (you mentioned interest in reinforcement learning last week), and a <strong>research space</strong> (recent RLHF papers). All composed dynamically, no upfront merging needed.
    </p>
    <div class="diagram"><span class="comment">Hydration request:</span>
<span class="label">user_id</span>:   <span class="node">"david"</span>
<span class="label">space_ids</span>: <span class="space">["memories", "research", "personality"]</span>
<span class="label">messages</span>:  <span class="node">"What papers should I read next?"</span>

<span class="comment">Result: nodes from all three spaces, ranked together</span>

<span class="node">[RLHF Survey 2025]</span>      <span class="space">research</span>      <span class="score">score: 0.91</span>
<span class="node">[Interested in RL]</span>       <span class="space">memories</span>      <span class="score">score: 0.87</span>
<span class="node">[Prefers practical]</span>      <span class="space">personality</span>   <span class="score">score: 0.72</span>
<span class="node">[Attention paper]</span>        <span class="space">research</span>      <span class="score">score: 0.65</span>
<span class="node">[PhD in CS]</span>              <span class="space">memories</span>      <span class="score">score: 0.41</span></div>
    <p>
        The system prompt is built from the top-scoring nodes across all spaces. The AI gets exactly the context it needs — personal knowledge, domain knowledge, and behavioral guidelines — all in one call.
    </p>
    <div class="section-divider"></div>
</div>

<!-- Retrieval Channels -->
<div class="section">
    <h2><span class="icon">&#x1F50D;</span> Retrieval Channels</h2>
    <p>
        NeuralGraph doesn't rely on a single retrieval method. It runs <strong>three concurrent channels</strong> and merges the results:
    </p>
    <table>
        <thead>
            <tr>
                <th>Channel</th>
                <th>How It Works</th>
                <th>What It Catches</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Trigger Matching</td>
                <td>Scans input for trigger phrases, weighted by strength</td>
                <td>Contextually relevant nodes based on conversational patterns</td>
            </tr>
            <tr>
                <td>Vector Search</td>
                <td>Embeds the input and finds semantically similar nodes</td>
                <td>Nodes with similar meaning that triggers might miss</td>
            </tr>
            <tr>
                <td>Graph Expansion</td>
                <td>Traverses edges from matched nodes to find connected knowledge</td>
                <td>Related context that wouldn't match on text alone</td>
            </tr>
        </tbody>
    </table>
    <p>
        All three channels run without an LLM call. Retrieval is pure computation — trigger matching, embedding lookup, graph traversal. <strong>Context retrieval takes milliseconds, not seconds.</strong> The only LLM call is the final response generation, which uses the context NeuralGraph assembled.
    </p>
    <div class="callout">
        <strong>Scoring:</strong> Each node's final score is a composite of trigger strength, match count, importance, recency, and coverage. Recently mentioned topics get a short-term boost (configurable, default 4 hours). Scoring weights are fully configurable per space type.
    </div>
    <div class="section-divider"></div>
</div>

<!-- Memory Lifecycle -->
<div class="section">
    <h2><span class="icon">&#x1F504;</span> Memory Lifecycle</h2>
    <p>Knowledge isn't static. NeuralGraph manages the full lifecycle of every node:</p>
    <ol class="flow-list">
        <li><strong>Extraction</strong> — conversations and content are ingested, an LLM extracts structured nodes, triggers, and edges into the graph</li>
        <li><strong>Reinforcement</strong> — when a node surfaces and is useful, its triggers strengthen and importance increases</li>
        <li><strong>Decay</strong> — nodes that haven't been accessed gradually lose importance. Configurable decay curves per space type</li>
        <li><strong>Consolidation</strong> — related nodes can be merged or summarized over time, keeping the graph dense and relevant</li>
        <li><strong>Deletion</strong> — nodes can be soft-deleted, removing them from retrieval while preserving graph structure</li>
    </ol>
    <p>
        The result is a graph that <strong>naturally reflects what matters now</strong>. Recent, frequently-referenced knowledge rises to the top. Old, unused knowledge fades. No manual curation needed.
    </p>
    <div class="section-divider"></div>
</div>

<!-- Feedback Loop -->
<div class="section page-break">
    <h2><span class="icon">&#x1F3AF;</span> Relevance Feedback</h2>
    <p>
        After every response, the AI rates which context was relevant and which wasn't. These ratings flow back to NeuralGraph as <strong>relevance feedback</strong>, adjusting trigger strengths in real time.
    </p>
    <div class="diagram"><span class="comment">User: "How's the weather?"</span>
<span class="comment">Hydration surfaces: [Weather preferences], [Q3 Launch], [Stress]</span>

<span class="comment">AI rates context:</span>
<span class="node">[Weather preferences]</span>  <span class="score">relevant      triggers +0.1</span>
<span class="node">[Q3 Launch]</span>             <span class="label">not_relevant  triggers -0.05</span>
<span class="node">[Stress]</span>                <span class="label">not_relevant  triggers -0.05</span>

<span class="comment">Next time someone asks about weather,</span>
<span class="comment">[Q3 Launch] is less likely to surface.</span></div>
    <p>
        <strong>No retraining. No embeddings recomputed.</strong> Just edge weight adjustments in the graph. The system gets more precise with every interaction.
    </p>
    <div class="section-divider"></div>
</div>

<!-- Comparison -->
<div class="section">
    <h2><span class="icon">&#x2696;</span> Compared to Other Approaches</h2>
    <table>
        <thead>
            <tr>
                <th>Approach</th>
                <th>Retrieval</th>
                <th>Structure</th>
                <th>Adapts Over Time</th>
                <th>Multi-Domain</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>RAG (vector only)</td>
                <td>Semantic similarity</td>
                <td>Flat text chunks</td>
                <td>No</td>
                <td>No</td>
            </tr>
            <tr>
                <td>Entity Knowledge Graphs</td>
                <td>Entity lookup</td>
                <td>Entity-relation triples</td>
                <td>No</td>
                <td>No</td>
            </tr>
            <tr>
                <td>Memory-augmented LLMs</td>
                <td>Conversation window</td>
                <td>Flat message history</td>
                <td>Recency only</td>
                <td>No</td>
            </tr>
            <tr>
                <td>Graph Memory Systems</td>
                <td>Graph relationships</td>
                <td>Memory graph with updates/extends</td>
                <td>Partial (rewrites)</td>
                <td>No</td>
            </tr>
            <tr class="highlight">
                <td>NeuralGraph</td>
                <td>Triggers + vectors + graph (concurrent)</td>
                <td>Typed nodes, weighted edges, configurable schemas</td>
                <td>Adaptive triggers, feedback, decay</td>
                <td>Multi-space composition</td>
            </tr>
        </tbody>
    </table>
    <p>
        Most existing approaches solve one piece of the problem. Vector RAG handles semantic recall but loses structure. Knowledge graphs preserve structure but can't adapt. Conversation windows are simple but forget everything outside the window. <strong>NeuralGraph combines structured storage, multi-channel retrieval, and continuous adaptation into a single platform</strong> — and lets applications compose across independent knowledge domains at query time.
    </p>
    <div class="section-divider"></div>
</div>

<!-- Architecture -->
<div class="section">
    <h2><span class="icon">&#x2699;</span> Architecture</h2>
    <p>
        NeuralGraph is a single Go binary backed by four stores:
    </p>
    <table>
        <thead>
            <tr>
                <th>Store</th>
                <th>Technology</th>
                <th>Purpose</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Content</td>
                <td>PostgreSQL</td>
                <td>Nodes, profiles, jobs, space configs</td>
            </tr>
            <tr>
                <td>Graph</td>
                <td>Memgraph</td>
                <td>Edges, triggers, graph traversal</td>
            </tr>
            <tr>
                <td>Vector</td>
                <td>Qdrant</td>
                <td>Embeddings for semantic search</td>
            </tr>
            <tr>
                <td>Cache</td>
                <td>Redis</td>
                <td>Sessions, rate limiting, hot data</td>
            </tr>
        </tbody>
    </table>
    <p>
        Embeddings run locally via HuggingFace TEI — no external API calls for vector operations. The API is stateless and horizontally scalable. Multi-tenancy is built in at every layer. Space types are defined in YAML — new domains can be added without code changes.
    </p>
    <p>
        Deployed on GKE Autopilot. Total infrastructure cost: <strong>~$95/month</strong> for API, databases, embeddings, and vector search.
    </p>
</div>

<div class="footer">
    NeuralGraph LLC &mdash; Confidential &mdash; david@neuralgraph.app
</div>

</body>
</html>
